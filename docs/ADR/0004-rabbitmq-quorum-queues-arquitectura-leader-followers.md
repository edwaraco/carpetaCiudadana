# ADR-0004: RabbitMQ Quorum Queues en Kubernetes con Cluster Operator

## Estado
**Aceptado** - 2025-11-05
**Actualizado** - 2025-11-05 (Migraci√≥n a Kubernetes)

## Contexto

El sistema Carpeta Ciudadana utiliza RabbitMQ como message broker central para implementar arquitectura event-driven (ver ADR-0003). Este sistema requiere alta disponibilidad y durabilidad de mensajes debido a:

1. **Escala Nacional**: ~55 millones de ciudadanos colombianos (RNF-06)
2. **Alta Disponibilidad Cr√≠tica**: 99.99% de uptime para Core Domain (RNF-01)
3. **Volumen de Transacciones**: Hasta 5 millones de transferencias de documentos al d√≠a (RNF-08)
4. **P√©rdida de Datos M√≠nima**: RPO < 5 minutos (RNF-03)
5. **Tolerancia a Fallos**: Sistema resiliente a fallo de una regi√≥n completa (RNF-04)
6. **Escalabilidad Horizontal**: Capacidad de agregar nodos sin l√≠mite te√≥rico (RNF-09)

### Problema de Configuraci√≥n Manual de Nodos

Al dise√±ar un cluster RabbitMQ, una aproximaci√≥n com√∫n pero **problem√°tica** es definir cada nodo expl√≠citamente en la configuraci√≥n:

```yaml
# ‚ùå ANTI-PATTERN: Configuraci√≥n no escalable
services:
  rabbitmq-node1:
    ...
  rabbitmq-node2:
    ...
  rabbitmq-node3:
    ...
  # ¬øQu√© pasa si necesitamos 50 nodos?
```

**Limitaciones de Configuraci√≥n Node-by-Node:**
- ‚ùå **No Escalable**: Requiere editar configuraci√≥n manualmente para cada nodo adicional
- ‚ùå **Overhead de Configuraci√≥n**: 50 nodos = 50 definiciones de servicio duplicadas
- ‚ùå **Propenso a Errores**: F√°cil olvidar configurar un par√°metro en alg√∫n nodo
- ‚ùå **Dif√≠cil de Mantener**: Cambios requieren actualizar N definiciones id√©nticas
- ‚ùå **Violaci√≥n RNF-09**: No permite escalado horizontal sin l√≠mite te√≥rico

**Impacto en Requisitos No Funcionales:**
- üî¥ **RNF-06 (55M ciudadanos)**: Cluster debe crecer con la demanda sin reescribir config
- üî¥ **RNF-08 (5M transacciones/d√≠a)**: Load puede requerir 10-50 nodos bajo picos
- üî¥ **RNF-09 (Escalado Horizontal)**: No se puede agregar nodos din√°micamente

### Soluci√≥n: Arquitectura Leader-Followers

La **arquitectura Leader-Followers** es un patr√≥n est√°ndar en sistemas distribuidos donde:

1. **Un nodo Leader** act√∫a como punto de entrada inicial y coordinador del cluster
2. **N nodos Followers** se unen din√°micamente al cluster liderado por el Leader
3. **Escalabilidad mediante replicaci√≥n de servicio** usando `docker compose scale` o `deploy.replicas`

**Caracter√≠sticas Clave:**
- ‚úÖ **Escalabilidad Ilimitada**: `docker compose up --scale rabbitmq-follower=50`
- ‚úÖ **Configuraci√≥n DRY**: Definir una vez el servicio follower, escalar N veces
- ‚úÖ **Mantenimiento Simplificado**: Cambios se propagan a todos los followers
- ‚úÖ **Cumple RNF-09**: Escalado horizontal sin editar archivos de configuraci√≥n
- ‚úÖ **Industry Standard**: Patr√≥n usado en Kubernetes, Kafka, Elasticsearch

```mermaid
%%{init: {'theme': 'neutral', "flowchart" : { "curve" : "basis" } } }%%
graph TB
    subgraph "RabbitMQ Cluster - Leader-Followers (Escalable)"
        Leader["üîµ Leader<br/>Nodo principal<br/>Coordinador Raft"]
        Follower1["‚ö™ Follower 1<br/>R√©plica sincronizada"]
        Follower2["‚ö™ Follower 2<br/>R√©plica sincronizada"]
        Follower3["‚ö™ Follower 3<br/>R√©plica sincronizada"]
        Follower4["‚ö™ Follower 4<br/>R√©plica sincronizada"]
        FollowerN["‚ö™ Follower N<br/>Escalable a 50+"]
    end
    
    Producer[Producer<br/>Spring Boot Service] -->|Publish| Leader
    Leader -->|Replicaci√≥n Raft| Follower1
    Leader -->|Replicaci√≥n Raft| Follower2
    Leader -->|Replicaci√≥n Raft| Follower3
    Leader -->|Replicaci√≥n Raft| Follower4
    Leader -.->|Escalable| FollowerN
    
    Consumer[Consumer<br/>Document Service] -->|Consume| Leader
    
    Scale["`docker compose up
    --scale rabbitmq-follower=50`"] -.->|Agregar nodos| FollowerN
    
    style Leader fill:#4a90e2,stroke:#2e5c8a,color:#fff
    style Follower1 fill:#e8f4f8,stroke:#4a90e2
    style Follower2 fill:#e8f4f8,stroke:#4a90e2
    style Follower3 fill:#e8f4f8,stroke:#4a90e2
    style Follower4 fill:#e8f4f8,stroke:#4a90e2
    style FollowerN fill:#e8f4f8,stroke:#4a90e2,stroke-dasharray: 5 5
    style Scale fill:#fff3cd,stroke:#ffc107
```

### Quorum Queues con Raft Consensus

Las **Quorum Queues** (RabbitMQ 3.8+) usan el algoritmo de consenso **Raft** para replicaci√≥n:

- ‚úÖ **Replicaci√≥n Autom√°tica**: Mensajes replicados a mayor√≠a de nodos (RF=2)
- ‚úÖ **Consistencia Fuerte**: Raft garantiza consenso distribuido linearizable
- ‚úÖ **Failover Autom√°tico**: Elecci√≥n de nuevo l√≠der en <5 segundos
- ‚úÖ **Sin P√©rdida de Mensajes**: ACK solo cuando persistido en quorum (mayor√≠a)
- ‚úÖ **Poison Message Handling**: Tracking de delivery count a nivel de cluster

### Pregunta de Dise√±o

**¬øC√≥mo debemos configurar el cluster RabbitMQ para soportar escalabilidad horizontal ilimitada mientras mantenemos alta disponibilidad y simplicidad operacional?**

## Decisi√≥n

Implementaremos **RabbitMQ con arquitectura Leader-Followers escalable** usando:

1. **1 servicio Leader** - Nodo principal que inicia el cluster
2. **1 servicio Follower (escalable)** - Servicio replicable que se une al Leader
3. **Quorum Queues con replication factor 2** - Para durabilidad de mensajes
4. **Configuraci√≥n inicial: 1 Leader + 4 Followers = 5 nodos totales**

### Fundamentos de la Decisi√≥n

#### 1. Por Qu√© Leader-Followers (No Node-by-Node)

**Comparaci√≥n de Enfoques:**

| Criterio | Node-by-Node | Leader-Followers |
|----------|--------------|------------------|
| **Escalabilidad** | ‚ùå Manual, editar config | ‚úÖ `--scale follower=N` |
| **Mantenimiento** | ‚ùå Actualizar N nodos | ‚úÖ Actualizar 1 definici√≥n |
| **Configuraci√≥n** | ‚ùå N √ó definiciones | ‚úÖ 2 definiciones (L+F) |
| **Cumple RNF-09** | ‚ùå No | ‚úÖ S√≠ |
| **Industry Standard** | ‚ö†Ô∏è Legacy approach | ‚úÖ Modern pattern |

**Ejemplo de Escalabilidad:**

```bash
# Cluster inicial (5 nodos)
docker compose up -d

# Escalar a 10 nodos (1 Leader + 9 Followers)
docker compose up -d --scale rabbitmq-follower=9

# Escalar a 50 nodos (1 Leader + 49 Followers)
docker compose up -d --scale rabbitmq-follower=49

# Sin editar docker-compose.yml ‚úÖ
```

#### 2. N√∫mero Inicial de Nodos: 5 (1 Leader + 4 Followers)

**Por qu√© 5 nodos inicialmente:**

- ‚úÖ **Consenso Raft**: 5 nodos toleran 2 fallos simult√°neos (quorum = 3)
- ‚úÖ **Balance Producci√≥n**: 5 nodos es est√°ndar para clusters productivos
- ‚úÖ **Redundancia Mejorada**: Mejor que 3 nodos para cargas cr√≠ticas
- ‚úÖ **Cumple RNF-04**: Tolera 2 fallos vs 1 fallo con 3 nodos

**F√≥rmula de Tolerancia Raft:**
```
Tolerancia a fallos = ‚åä(N - 1) / 2‚åã

N=3 ‚Üí Tolerancia = ‚åä(3-1)/2‚åã = 1 nodo
N=5 ‚Üí Tolerancia = ‚åä(5-1)/2‚åã = 2 nodos ‚úÖ (MEJOR)
N=7 ‚Üí Tolerancia = ‚åä(7-1)/2‚åã = 3 nodos (overhead excesivo para inicio)
```

**Configuraci√≥n Escalable:**
- **Desarrollo**: 1 Leader + 2 Followers (3 nodos, recursos limitados)
- **Staging**: 1 Leader + 4 Followers (5 nodos, balanceado)
- **Producci√≥n**: 1 Leader + 6-49 Followers (7-50 nodos, seg√∫n carga)

#### 3. Replication Factor: 2

**Por qu√© RF=2:**
- ‚úÖ **Durabilidad Garantizada**: Mensajes en 2+ nodos antes de ACK
- ‚úÖ **Balance Latencia-Durabilidad**: M√°s r√°pido que RF=3, m√°s seguro que RF=1
- ‚úÖ **Cumple RNF-03**: RPO < 5 minutos; sin p√©rdida si 1+ nodos operativos
- ‚úÖ **Overhead Aceptable**: 2x almacenamiento vs 1x (classic queues)

| RF | Durabilidad | Latencia | Almacenamiento | Decisi√≥n |
|----|-------------|----------|----------------|----------|
| 1  | ‚ùå Baja     | ‚ö° R√°pida | 1x             | ‚ùå Rechazado |
| 2  | ‚úÖ Alta     | ‚ö° Buena  | 2x             | ‚úÖ **Seleccionado** |
| 3  | ‚úÖ Muy Alta | ‚ö†Ô∏è Lenta  | 3x             | ‚ö†Ô∏è Excesivo |

### Configuraci√≥n T√©cnica

#### Docker Compose Escalable

```yaml
services:
  # ==========================================
  # RabbitMQ Leader - Nodo principal
  # ==========================================
  rabbitmq-leader:
    image: rabbitmq:3.12-management
    container_name: rabbitmq-leader
    hostname: rabbitmq-leader
    ports:
      - "5672:5672"   # AMQP
      - "15672:15672" # Management UI
    environment:
      - RABBITMQ_ERLANG_COOKIE=SWQOKODSQALRPCLNMEQG
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin123
      - RABBITMQ_NODE_NAME=rabbit@rabbitmq-leader
    volumes:
      - rabbitmq-leader-data:/var/lib/rabbitmq
    networks:
      - app-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "check_port_connectivity"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s

  # ==========================================
  # RabbitMQ Followers - Nodos escalables
  # ==========================================
  rabbitmq-follower:
    image: rabbitmq:3.12-management
    hostname: rabbitmq-follower
    environment:
      - RABBITMQ_ERLANG_COOKIE=SWQOKODSQALRPCLNMEQG
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin123
      - RABBITMQ_LEADER_HOST=rabbitmq-leader
    volumes:
      - ./rabbitmq/cluster-entrypoint.sh:/usr/local/bin/cluster-entrypoint.sh:ro
    entrypoint: ["/usr/local/bin/cluster-entrypoint.sh"]
    networks:
      - app-network
    depends_on:
      rabbitmq-leader:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "check_port_connectivity"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      replicas: 4  # Escalable con --scale rabbitmq-follower=N

volumes:
  rabbitmq-leader-data:  # Solo el Leader necesita volume nombrado

networks:
  app-network:
    driver: bridge
```

#### Script de Clustering Din√°mico

```bash
#!/bin/bash
set -e

echo "Starting RabbitMQ follower node..."

# Obtener hostname del l√≠der desde variable de entorno
LEADER_HOST="${RABBITMQ_LEADER_HOST:-rabbitmq-leader}"
echo "Leader host: $LEADER_HOST"

# Iniciar RabbitMQ
rabbitmq-server -detached

# Esperar a que RabbitMQ est√© listo
echo "Waiting for RabbitMQ to be ready..."
timeout 90 bash -c 'until rabbitmq-diagnostics -q ping; do sleep 2; done'

# Unirse al cluster del l√≠der
echo "Joining cluster..."
rabbitmqctl stop_app
rabbitmqctl reset
rabbitmqctl join_cluster rabbit@${LEADER_HOST}
rabbitmqctl start_app

echo "Successfully joined cluster!"
rabbitmqctl cluster_status

# Mantener contenedor vivo
exec rabbitmq-server
```

#### Configuraci√≥n de Quorum Queues en Spring Boot

```java
@Configuration
public class RabbitMQQuorumConfig {

    public static final String EXCHANGE_NAME = "documento.events";
    public static final String DELETION_QUEUE = "documento.deletion.queue";

    @Bean
    public TopicExchange documentoExchange() {
        return ExchangeBuilder
            .topicExchange(EXCHANGE_NAME)
            .durable(true)
            .build();
    }

    /**
     * Quorum Queue con arquitectura Leader-Followers.
     * El cluster escala din√°micamente; la queue se replica autom√°ticamente
     * a todos los nodos activos seg√∫n Raft consensus.
     */
    @Bean
    public Queue deletionQueue() {
        return QueueBuilder
            .durable(DELETION_QUEUE)
            .withArgument("x-queue-type", "quorum")  // üîë Quorum Queue
            .withArgument("x-quorum-initial-group-size", 5)  // üîë 1 Leader + 4 Followers
            .withArgument("x-delivery-limit", 3)
            .withArgument("x-dead-letter-exchange", EXCHANGE_NAME)
            .withArgument("x-dead-letter-routing-key", "documento.deletion.dlq")
            .build();
    }

    @Bean
    public Binding deletionBinding() {
        return BindingBuilder
            .bind(deletionQueue())
            .to(documentoExchange())
            .with("documento.deletion.requested");
    }
}
```

#### Spring Boot Connection

```yaml
# application-docker.yml
spring:
  rabbitmq:
    # Conectar al Leader; RabbitMQ balancea autom√°ticamente
    host: rabbitmq-leader
    port: 5672
    username: admin
    password: admin123
    
    # Publisher Confirms
    publisher-confirm-type: correlated
    publisher-returns: true
    
    # Listener configuration
    listener:
      simple:
        acknowledge-mode: manual
        prefetch: 10
        retry:
          enabled: true
          max-attempts: 3
```

## Consecuencias

### Positivas

- ‚úÖ **RNF-09 (Escalabilidad Horizontal)**: Cluster escala de 3 a 50+ nodos sin cambiar config
- ‚úÖ **RNF-01 (Disponibilidad 99.99%)**: 5 nodos toleran 2 fallos simult√°neos
- ‚úÖ **RNF-03 (RPO < 5 minutos)**: RF=2 garantiza mensajes en 2+ nodos
- ‚úÖ **RNF-04 (Tolerancia a Fallos)**: Tolera 2 fallos con 5 nodos (mejor que 1 con 3)
- ‚úÖ **Mantenimiento Simplificado**: Cambios solo en 1 definici√≥n de servicio follower
- ‚úÖ **Industry Standard**: Patr√≥n usado en Kubernetes, Kafka, Elasticsearch
- ‚úÖ **Flexibility**: Escalar up/down en segundos seg√∫n carga
- ‚úÖ **DRY Principle**: No repetir configuraci√≥n N veces

### Negativas

- ‚ö†Ô∏è **Dependencia en Leader**: Si el Leader falla, Raft elige nuevo l√≠der (5-10s downtime)
    - **Mitigaci√≥n**: Raft failover autom√°tico en <5 segundos
    
- ‚ö†Ô∏è **Overhead de Replicaci√≥n**: RF=2 implica 2x almacenamiento
    - **Aceptable**: Trade-off necesario para durabilidad
    
- ‚ö†Ô∏è **Configuraci√≥n Inicial M√°s Compleja**: Requiere script de clustering
    - **Mitigaci√≥n**: Script reutilizable, documentado y probado

### Riesgos

- üî¥ **Split Brain con N nodos**: Red particionada podr√≠a crear sub-clusters
    - **Mitigaci√≥n**: Raft previene split brain; solo cluster con mayor√≠a opera

- üî¥ **P√©rdida de Mayor√≠a**: Si fallan 3+ de 5 nodos, cluster se vuelve read-only
    - **Mitigaci√≥n**: Monitoreo proactivo, alertas, auto-scaling en cloud

- üî¥ **Latencia con Muchos Nodos**: 50 nodos pueden incrementar latencia Raft
    - **Mitigaci√≥n**: Medir latencia, ajustar n√∫mero √≥ptimo seg√∫n carga real

## Alternativas Consideradas

### Opci√≥n 1: Configuraci√≥n Node-by-Node (3 Nodos Expl√≠citos)
```yaml
services:
  rabbitmq-node1: ...
  rabbitmq-node2: ...
  rabbitmq-node3: ...
```
- **Rechazo**: No escalable, viola RNF-09, overhead de configuraci√≥n para N>10 nodos

### Opci√≥n 2: Kubernetes StatefulSet desde el Inicio
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: rabbitmq
spec:
  replicas: 5
```
- **Rechazo**: Overkill para desarrollo local, requiere cluster K8s, curva de aprendizaje

### Opci√≥n 3: RabbitMQ Cluster Operator (Kubernetes)
- **Rechazo para Desarrollo**: Complejidad innecesaria para ambiente local
- **Aceptado para Producci√≥n**: Ver ADR-0005 para migraci√≥n futura

### Opci√≥n 4: Classic Queues con HA Mirroring
- **Rechazo**: Deprecado en RabbitMQ 3.8+, menor consistencia que Quorum Queues

## M√©tricas y Monitoreo

### M√©tricas Clave de Cluster Escalable

```java
@Component
public class RabbitMQClusterMetrics {

    @Scheduled(fixedDelay = 60000)
    public void captureClusterMetrics() {
        // N√∫mero din√°mico de nodos en el cluster
        meterRegistry.gauge("rabbitmq.cluster.nodes.total", getClusterNodesCount());
        
        // Followers activos
        meterRegistry.gauge("rabbitmq.cluster.followers.active", getActiveFollowersCount());
        
        // Estado de quorum (cu√°ntos nodos tienen r√©plica)
        meterRegistry.gauge("rabbitmq.quorum.members.online", 
            () -> getQuorumMembersOnline("documento.deletion.queue"));
    }
}
```

### Alertas Cr√≠ticas

1. **Cluster con <3 nodos activos**: P√©rdida de mayor√≠a Raft
2. **Followers <2**: Solo Leader + 1 follower = sin redundancia real
3. **Quorum Queue sin mayor√≠a**: Mensajes no pueden ser escritos
4. **Latencia Raft >100ms**: Indicador de problema de red o CPU

## Validaci√≥n y Testing

### Test de Escalabilidad

```bash
# 1. Iniciar con configuraci√≥n m√≠nima (3 nodos)
docker compose up -d --scale rabbitmq-follower=2

# 2. Verificar cluster
docker exec rabbitmq-leader rabbitmqctl cluster_status
# Expected: 1 Leader + 2 Followers = 3 nodos

# 3. Escalar a 10 nodos
docker compose up -d --scale rabbitmq-follower=9

# 4. Verificar escalado
docker exec rabbitmq-leader rabbitmqctl cluster_status
# Expected: 1 Leader + 9 Followers = 10 nodos

# 5. Escalar a 50 nodos (stress test)
docker compose up -d --scale rabbitmq-follower=49

# 6. Verificar cluster sigue funcional
docker exec rabbitmq-leader rabbitmqctl list_queues name type state
```

### Test de Failover

```bash
# 1. Detener el Leader
docker stop rabbitmq-leader

# 2. Verificar elecci√≥n de nuevo l√≠der (<10 segundos)
docker exec rabbitmq-follower-1 rabbitmqctl cluster_status

# 3. Verificar queue sigue operacional
docker exec rabbitmq-follower-1 rabbitmqctl list_queues name state

# 4. Reiniciar antiguo Leader
docker start rabbitmq-leader

# 5. Verificar se une como Follower
docker exec rabbitmq-leader rabbitmqctl cluster_status
```

## Implementaci√≥n Faseada

### Fase 1: Cluster Escalable Base (Semana 1)
- [x] Configurar servicio Leader en docker-compose.yml
- [x] Configurar servicio Follower escalable
- [x] Crear script de clustering din√°mico
- [x] Validar escalado: 3, 5, 10 nodos

### Fase 2: Quorum Queues (Semana 2)
- [ ] Implementar RabbitMQQuorumConfig con x-queue-type=quorum
- [ ] Configurar x-quorum-initial-group-size din√°mico
- [ ] Testing de replicaci√≥n con RF=2

### Fase 3: Monitoreo Escalabilidad (Semana 3)
- [ ] M√©tricas de cluster din√°mico (nodes, followers)
- [ ] Alertas para p√©rdida de nodos
- [ ] Dashboard Grafana con n√∫mero de nodos activos

### Fase 4: Load Testing (Semana 4)
- [ ] Test con 10K msg/s en cluster de 5 nodos
- [ ] Test con 10K msg/s en cluster de 10 nodos
- [ ] Test con 10K msg/s en cluster de 50 nodos
- [ ] Medir impacto de escalado en latencia

## Referencias

- [RabbitMQ Quorum Queues](https://www.rabbitmq.com/quorum-queues.html)
- [Raft Consensus Algorithm](https://raft.github.io/)
- [RabbitMQ Clustering Guide](https://www.rabbitmq.com/clustering.html)
- [Docker Compose Scale](https://docs.docker.com/compose/compose-file/deploy/#replicas)
- [Leader-Followers Pattern (Microservices)](https://microservices.io/patterns/deployment/service-per-container.html)
- ADR-0003: Eliminaci√≥n de Documentos con Arquitectura Event-Driven
- ADR-0005: Ubicaci√≥n de RabbitMQ en Docker Compose
- RNF-01, RNF-03, RNF-04, RNF-06, RNF-08, RNF-09

## Notas Adicionales

1. **Escalabilidad en Producci√≥n**: En Kubernetes, usar RabbitMQ Cluster Operator con StatefulSet
2. **N√∫mero √ìptimo de Nodos**: Medir en producci√≥n; 5-10 nodos suele ser √≥ptimo para la mayor√≠a de casos
3. **Erlang Cookie**: Mismo cookie en todos los nodos para permitir clustering
4. **Network Latency**: Mantener nodos en la misma regi√≥n para minimizar latencia Raft
5. **Backup Strategy**: Solo Leader necesita backups peri√≥dicos; Followers son replicables

---

**Fecha**: 2025-11-05  
**Autores**: Equipo Carpeta Ciudadana  
**Revisores**: Pendiente

---

## Actualizaci√≥n: Migraci√≥n a Kubernetes (2025-11-05)

### Nuevo Contexto

El sistema ha evolucionado desde un ambiente de desarrollo local con Docker Compose hacia un despliegue production-ready en Kubernetes. Esta migraci√≥n permite:

1. **Escalabilidad Autom√°tica**: Kubernetes gestiona el ciclo de vida de los pods
2. **Alta Disponibilidad Real**: Distribuci√≥n en m√∫ltiples nodos f√≠sicos
3. **Gesti√≥n Declarativa**: Infraestructura como c√≥digo con manifiestos YAML
4. **Peer Discovery Autom√°tico**: Plugin kubernetes_peer_discovery integrado

### Nueva Decisi√≥n: RabbitMQ Cluster Operator

Implementaremos RabbitMQ usando el **RabbitMQ Cluster Operator** en Kubernetes:

#### Arquitectura Kubernetes

```mermaid
graph TB
    subgraph "Kubernetes Cluster"
        subgraph "RabbitMQ StatefulSet"
            Node0["üîµ carpeta-rabbitmq-server-0<br/>Seed Node (Ordinal 0)<br/>PVC: data-0"]
            Node1["‚ö™ carpeta-rabbitmq-server-1<br/>Member Node<br/>PVC: data-1"]
            Node2["‚ö™ carpeta-rabbitmq-server-2<br/>Member Node<br/>PVC: data-2"]
        end
        
        Operator["RabbitMQ Cluster Operator<br/>Gesti√≥n autom√°tica"]
        Service["Service: carpeta-rabbitmq<br/>LoadBalancer AMQP"]
        
        PV0["PersistentVolume<br/>data-0: 10Gi"]
        PV1["PersistentVolume<br/>data-1: 10Gi"]
        PV2["PersistentVolume<br/>data-2: 10Gi"]
    end
    
    Node0 -->|Raft Consensus| Node1
    Node0 -->|Raft Consensus| Node2
    Node1 -->|Raft Sync| Node2
    
    Node0 --- PV0
    Node1 --- PV1
    Node2 --- PV2
    
    Operator -.->|Manage| Node0
    Operator -.->|Manage| Node1
    Operator -.->|Manage| Node2
    
    Service --> Node0
    Service --> Node1
    Service --> Node2
    
    Apps[Spring Boot Services] -->|AMQP| Service
    
    style Node0 fill:#4a90e2,stroke:#2e5c8a,color:#fff
    style Node1 fill:#e8f4f8,stroke:#4a90e2
    style Node2 fill:#e8f4f8,stroke:#4a90e2
    style Operator fill:#f0ad4e,stroke:#d58512
```

#### Configuraci√≥n del Cluster

**Cluster Configuration** (RabbitmqCluster Custom Resource):

```yaml
apiVersion: rabbitmq.com/v1beta1
kind: RabbitmqCluster
metadata:
  name: carpeta-rabbitmq
  namespace: carpeta-ciudadana
spec:
  replicas: 3  # M√≠nimo para Quorum Queues
  
  persistence:
    storageClassName: standard
    storage: 10Gi  # Cada nodo tiene su propio volumen
  
  rabbitmq:
    additionalConfig: |
      # Peer Discovery en Kubernetes
      cluster_formation.peer_discovery_backend = kubernetes
      cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
      cluster_formation.k8s.address_type = hostname
      
      # Seed node: pod con ordinal 0 (carpeta-rabbitmq-server-0)
      # Solo este pod puede formar un nuevo cluster
      # Todos los dem√°s se unen a √©l
      
      # Quorum Queue defaults
      vm_memory_high_watermark.relative = 0.6
      disk_free_limit.absolute = 2GB
    
    additionalPlugins:
      - rabbitmq_management
      - rabbitmq_prometheus
      - rabbitmq_peer_discovery_k8s
```

### Peer Discovery en Kubernetes

Desde RabbitMQ 4.1, el plugin `rabbitmq_peer_discovery_k8s` implementa un modelo simplificado:

**Caracter√≠sticas Clave:**

1. **Seed Node √önico**: Solo el pod con ordinal m√°s bajo (`-0`) puede formar un nuevo cluster
2. **Join Autom√°tico**: Todos los dem√°s pods se unen al seed node
3. **Hostname-Based**: Discovery basado en hostnames del StatefulSet
4. **Sin Configuraci√≥n Manual**: El Cluster Operator configura todo autom√°ticamente

**Configuraci√≥n Autom√°tica**:
```ini
cluster_formation.peer_discovery_backend = kubernetes
cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
cluster_formation.k8s.address_type = hostname
cluster_formation.k8s.ordinal_start = 0  # Default
```

Si es necesario forzar un seed node espec√≠fico (muy inusual):
```ini
cluster_formation.k8s.seed_node = rabbit@carpeta-rabbitmq-server-0
```

### Quorum Queues con Replication Factor 2

**Configuraci√≥n de Queues**:

```java
@Bean
public Queue documentDeletionQueue() {
    return QueueBuilder
        .durable("documento.deletion.queue")
        .withArgument("x-queue-type", "quorum")
        .withArgument("x-quorum-initial-group-size", 3)  // 3 nodos
        .withArgument("x-delivery-limit", 3)
        .build();
}
```

**Replication Factor**: Con 3 nodos y `x-quorum-initial-group-size=3`, el quorum es 2 nodos (‚åà(3+1)/2‚åâ = 2).

**Tolerancia a Fallos**:
- ‚úÖ Cluster funciona con 2 de 3 nodos activos
- ‚úÖ Mensajes replicados en al menos 2 nodos antes de ACK
- ‚úÖ Failover autom√°tico si falla 1 nodo
- ‚ùå Read-only si fallan 2+ nodos

### kubectl Plugin para RabbitMQ

**Instalaci√≥n via krew**:

```bash
# Instalar krew (plugin manager)
(
  set -x; cd "$(mktemp -d)" &&
  OS="$(uname | tr '[:upper:]' '[:lower:]')" &&
  ARCH="$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\(arm\)\(64\)\?.*/\1\2/' -e 's/aarch64$/arm64/')" &&
  KREW="krew-${OS}_${ARCH}" &&
  curl -fsSLO "https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz" &&
  tar zxvf "${KREW}.tar.gz" &&
  ./"${KREW}" install krew
)

# Agregar al PATH
export PATH="${KREW_ROOT:-$HOME/.krew}/bin:$PATH"

# Instalar plugin RabbitMQ
kubectl krew install rabbitmq

# Verificar
kubectl rabbitmq version
```

**Comandos √ötiles**:

```bash
# Ver estado del cluster
kubectl rabbitmq get carpeta-rabbitmq -n carpeta-ciudadana

# Exportar definitions
kubectl rabbitmq export-definitions carpeta-rabbitmq -n carpeta-ciudadana

# Acceder a Management UI
kubectl rabbitmq manage carpeta-rabbitmq -n carpeta-ciudadana
```

### Operator Environment Variables

El Cluster Operator puede configurarse mediante variables de entorno:

```bash
# Editar deployment del operator
kubectl -n rabbitmq-system edit deployment rabbitmq-cluster-operator

# Agregar variables de entorno
env:
- name: OPERATOR_SCOPE_NAMESPACE
  value: "carpeta-ciudadana"  # Limitar a namespace espec√≠fico
- name: DEFAULT_RABBITMQ_IMAGE
  value: "rabbitmq:3.13-management"
```

**Variables Disponibles**:
- `OPERATOR_SCOPE_NAMESPACE`: Namespaces a gestionar (default: todos)
- `DEFAULT_RABBITMQ_IMAGE`: Imagen por defecto
- `DEFAULT_IMAGE_PULL_SECRETS`: Secrets para im√°genes privadas
- `CONTROL_RABBITMQ_IMAGE`: Auto-upgrade de im√°genes (‚ö†Ô∏è experimental)

### Persistencia: Shared Volume con Carpetas por Nodo

Cada pod del StatefulSet tiene su propio PersistentVolumeClaim:

```yaml
# Creado autom√°ticamente por el StatefulSet
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: persistence-carpeta-rabbitmq-server-0
spec:
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 10Gi
```

**Ventajas**:
- ‚úÖ Cada nodo tiene almacenamiento aislado
- ‚úÖ Vol√∫menes persisten si el pod se reinicia
- ‚úÖ Compatible con dynamic provisioning (AWS EBS, GCP PD, etc.)

### Escalado del Cluster

```bash
# Escalar a 5 nodos
kubectl patch rabbitmqcluster carpeta-rabbitmq -n carpeta-ciudadana \
  --type merge -p '{"spec":{"replicas":5}}'

# Verificar escalado
kubectl get pods -n carpeta-ciudadana -l app.kubernetes.io/name=carpeta-rabbitmq

# Ver cluster status
kubectl exec -n carpeta-ciudadana carpeta-rabbitmq-server-0 -- \
  rabbitmqctl cluster_status
```

## Nuevas Consecuencias

### Positivas Adicionales

- ‚úÖ **Auto-healing**: Kubernetes recrea pods fallidos autom√°ticamente
- ‚úÖ **Rolling Updates**: Actualizaciones sin downtime
- ‚úÖ **Resource Management**: Limits y requests para cada pod
- ‚úÖ **Service Discovery**: DNS autom√°tico en el cluster
- ‚úÖ **Monitoring**: Integraci√≥n con Prometheus nativo
- ‚úÖ **Peer Discovery Simplificado**: Plugin kubernetes configura todo
- ‚úÖ **Gesti√≥n Declarativa**: GitOps-friendly con manifiestos

### Negativas Adicionales

- ‚ö†Ô∏è **Complejidad Operacional**: Requiere conocimiento de Kubernetes
- ‚ö†Ô∏è **Dependencia del Operator**: Cluster depende del Cluster Operator funcionando
- ‚ö†Ô∏è **Costo de Recursos**: Kubernetes tiene overhead de management

### Mitigaciones

- üìö **Documentaci√≥n Completa**: Ver `services/rabbitmq-service/README.md`
- üîß **kubectl Plugin**: Simplifica operaciones comunes
- üìä **Monitoring**: Prometheus + Grafana para visibilidad
- üöÄ **CI/CD**: Automatizar despliegues y validaciones

## Referencias Adicionales

### RabbitMQ en Kubernetes

- [RabbitMQ Kubernetes Operator Overview](https://www.rabbitmq.com/kubernetes/operator/operator-overview)
- [RabbitMQ Cluster Formation](https://www.rabbitmq.com/docs/cluster-formation)
- [Peer Discovery on Kubernetes](https://www.rabbitmq.com/docs/cluster-formation#peer-discovery-k8s)
- [RabbitMQ Quorum Queues](https://www.rabbitmq.com/docs/quorum-queues)

### Tooling

- [kubectl rabbitmq Plugin](https://www.rabbitmq.com/kubernetes/operator/kubectl-plugin)
- [Configure Operator Defaults](https://www.rabbitmq.com/kubernetes/operator/configure-operator-defaults)
- [krew - kubectl Plugin Manager](https://krew.sigs.k8s.io/)

### Ejemplos

- [DIY Kubernetes Examples - Minikube](https://github.com/rabbitmq/diy-kubernetes-examples/tree/master/minikube)

### ADRs Relacionados

- ADR-0003: Eliminaci√≥n de Documentos Event-Driven
- ADR-0005: Migraci√≥n de Docker Compose a Kubernetes

---

**√öltima actualizaci√≥n**: 2025-11-05  
**Migraci√≥n completada a**: Kubernetes con RabbitMQ Cluster Operator
