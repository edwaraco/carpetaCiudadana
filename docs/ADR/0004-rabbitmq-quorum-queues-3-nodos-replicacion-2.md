# ADR-0004: Configuraci√≥n de RabbitMQ con Quorum Queues (3 Nodos, Replicaci√≥n Factor 2)

## Estado
**Aceptado** - 2025-11-05

## Contexto

El sistema Carpeta Ciudadana utiliza RabbitMQ como message broker central para implementar arquitectura event-driven (ver ADR-0003). Este sistema requiere alta disponibilidad y durabilidad de mensajes debido a:

1. **Escala Nacional**: ~55 millones de ciudadanos colombianos (RNF-06)
2. **Alta Disponibilidad Cr√≠tica**: 99.99% de uptime para Core Domain (RNF-01)
3. **Volumen de Transacciones**: Hasta 5 millones de transferencias de documentos al d√≠a (RNF-08)
4. **P√©rdida de Datos M√≠nima**: RPO < 5 minutos (RNF-03)
5. **Tolerancia a Fallos**: Sistema resiliente a fallo de una regi√≥n completa (RNF-04)

### Problema de Queues Cl√°sicas

El enfoque inicial de ADR-0003 propone RabbitMQ con queues cl√°sicas (durable), pero estas presentan limitaciones cr√≠ticas:

**Limitaciones de Classic Queues:**
- ‚ùå **Single Point of Failure**: La queue reside en un solo nodo; si ese nodo falla, los mensajes no procesados se pierden hasta que el nodo se recupere
- ‚ùå **Sin Replicaci√≥n Autom√°tica**: Requiere configuraci√≥n manual de HA Mirroring (deprecado en RabbitMQ 3.8+)
- ‚ùå **Consistencia D√©bil**: Los mirrors pueden estar desincronizados, causando p√©rdida de mensajes en failover
- ‚ùå **Overhead de Master-Slave**: El nodo master es un cuello de botella para escrituras

**Impacto en Requisitos No Funcionales:**
- üî¥ **RNF-01 (Disponibilidad 99.99%)**: Queue cl√°sica no tolera fallo de nodo sin p√©rdida de disponibilidad
- üî¥ **RNF-03 (RPO < 5 minutos)**: Potencial p√©rdida de mensajes en memoria al momento del fallo
- üî¥ **RNF-04 (Tolerancia a Fallos)**: No es resiliente a fallo de un nodo individual del cluster

### Quorum Queues: La Soluci√≥n

Las **Quorum Queues** (introducidas en RabbitMQ 3.8+) son un tipo de queue dise√±ado espec√≠ficamente para **alta disponibilidad y durabilidad** mediante el algoritmo de consenso **Raft**.

**Caracter√≠sticas Clave:**
- ‚úÖ **Replicaci√≥n Autom√°tica**: Los mensajes se replican en m√∫ltiples nodos del cluster
- ‚úÖ **Consistencia Fuerte**: Usa Raft para garantizar consenso distribuido
- ‚úÖ **Failover Autom√°tico**: Si el l√≠der falla, Raft elige autom√°ticamente un nuevo l√≠der de entre los followers
- ‚úÖ **Sin P√©rdida de Mensajes**: Los mensajes solo se confirman cuando est√°n persistidos en la mayor√≠a de r√©plicas
- ‚úÖ **Poison Message Handling**: Mejor manejo de mensajes que fallan repetidamente

**Algoritmo Raft:**
```mermaid
%%{init: {'theme': 'neutral', "flowchart" : { "curve" : "basis" } } }%%
graph TB
    subgraph "RabbitMQ Cluster - Quorum Queue (3 Nodos)"
        Leader["üîµ Nodo 1 (Leader)<br/>Acepta escrituras"]
        Follower1["‚ö™ Nodo 2 (Follower)<br/>R√©plica sincronizada"]
        Follower2["‚ö™ Nodo 3 (Follower)<br/>R√©plica sincronizada"]
    end
    
    Producer[Producer<br/>Spring Boot Service] -->|1. Publish mensaje| Leader
    Leader -->|2. Replica| Follower1
    Leader -->|2. Replica| Follower2
    Follower1 -.->|3. ACK| Leader
    Follower2 -.->|3. ACK| Leader
    Leader -.->|4. Confirm (mayor√≠a alcanzada)| Producer
    
    Consumer[Consumer<br/>Document Deletion Service] -->|5. Consume| Leader
    
    style Leader fill:#4a90e2,stroke:#2e5c8a,color:#fff
    style Follower1 fill:#e8f4f8,stroke:#4a90e2
    style Follower2 fill:#e8f4f8,stroke:#4a90e2
```

### Pregunta de Dise√±o

**¬øQu√© configuraci√≥n de Quorum Queues (n√∫mero de nodos y replication factor) debemos usar para cumplir con los requisitos de alta disponibilidad, durabilidad y eficiencia del sistema Carpeta Ciudadana?**

## Decisi√≥n

Implementaremos **RabbitMQ Quorum Queues con 3 nodos y replication factor de 2** para todas las queues cr√≠ticas del Core Domain.

### Fundamentos de la Decisi√≥n

#### 1. N√∫mero de Nodos: 3

**Por qu√© 3 nodos:**
- ‚úÖ **Consenso Raft**: Raft requiere mayor√≠a (quorum) para tomar decisiones. Con 3 nodos, la mayor√≠a es 2, permitiendo tolerar 1 fallo
- ‚úÖ **Balance Costo-Beneficio**: M√≠nimo n√∫mero de nodos para alta disponibilidad real sin overhead excesivo
- ‚úÖ **Tolerancia a Fallo**: Puede perder 1 nodo y seguir operando (2 nodos = mayor√≠a)
- ‚úÖ **Cumple RNF-04**: Sistema resiliente a fallo de un nodo completo

**F√≥rmula de Tolerancia:**
```
Tolerancia a fallos = ‚åä(N - 1) / 2‚åã
Donde N = n√∫mero de nodos

N=3 ‚Üí Tolerancia = ‚åä(3-1)/2‚åã = ‚åä2/2‚åã = 1 nodo
N=5 ‚Üí Tolerancia = ‚åä(5-1)/2‚åã = ‚åä4/2‚åã = 2 nodos (overhead excesivo para nuestro caso)
```

**Por qu√© NO m√°s nodos:**
- ‚ö†Ô∏è **Overhead de Consenso**: M√°s nodos = m√°s mensajes de coordinaci√≥n Raft = mayor latencia
- ‚ö†Ô∏è **Costo Infraestructura**: 5 nodos = 67% m√°s recursos que 3 nodos
- ‚ö†Ô∏è **Complejidad Operacional**: M√°s nodos = m√°s superficie de fallo, m√°s mantenimiento

#### 2. Replication Factor: 2

**Por qu√© replication factor 2:**
- ‚úÖ **Durabilidad Garantizada**: Cada mensaje est√° persistido en 2 nodos (l√≠der + 1 follower)
- ‚úÖ **Balance Durabilidad-Performance**: Los mensajes se confirman cuando 2 nodos han persistido (mayor√≠a de 3)
- ‚úÖ **Cumple RNF-03**: RPO < 5 minutos; los mensajes no se pierden mientras 2+ nodos est√©n operativos
- ‚úÖ **Eficiencia de Escritura**: Menor latencia que replication factor 3 (no necesita esperar a todos los nodos)

**Tabla Comparativa:**

| Replication Factor | Nodos Requeridos | Durabilidad | Latencia Escritura | Overhead Almacenamiento | Decisi√≥n |
|--------------------|------------------|-------------|-------------------|------------------------|----------|
| 1 (cl√°sica)        | 1 solo nodo      | ‚ùå Baja     | ‚ö° Muy r√°pida     | 1x (m√≠nimo)            | ‚ùå Rechazada |
| 2                  | 2 de 3 nodos     | ‚úÖ Alta     | ‚ö° R√°pida         | 2x                     | ‚úÖ **Seleccionada** |
| 3                  | 3 de 3 nodos     | ‚úÖ Muy Alta | ‚ö†Ô∏è M√°s lenta      | 3x (m√°ximo)            | ‚ö†Ô∏è Excesiva |

**An√°lisis de Consenso:**
```
Cluster de 3 nodos con RF=2:
- L√≠der (Nodo 1): Siempre tiene el mensaje
- Follower activo (Nodo 2): Recibe r√©plica del l√≠der
- Follower activo (Nodo 3): Recibe r√©plica del l√≠der

Quorum de escritura = 2 (mayor√≠a de 3)
‚Üí Mensaje se confirma cuando L√≠der + 1 Follower han persistido
‚Üí Latencia optimizada (no espera al tercer nodo)
‚Üí Durabilidad garantizada (2 copias f√≠sicas)
```

### Configuraci√≥n T√©cnica

#### Docker Compose para RabbitMQ Cluster

```yaml
version: '3.8'

services:
  # Nodo 1 - RabbitMQ Leader (inicialmente)
  rabbitmq-node1:
    image: rabbitmq:3.12-management
    container_name: carpeta-rabbitmq-node1
    hostname: rabbitmq-node1
    ports:
      - "5672:5672"   # AMQP
      - "15672:15672" # Management UI
    environment:
      - RABBITMQ_ERLANG_COOKIE=SWQOKODSQALRPCLNMEQG
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin123
      - RABBITMQ_NODE_NAME=rabbit@rabbitmq-node1
    volumes:
      - rabbitmq-node1-data:/var/lib/rabbitmq
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "check_port_connectivity"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Nodo 2 - RabbitMQ Follower
  rabbitmq-node2:
    image: rabbitmq:3.12-management
    container_name: carpeta-rabbitmq-node2
    hostname: rabbitmq-node2
    ports:
      - "5673:5672"   # AMQP (puerto alternativo para host)
      - "15673:15672" # Management UI (puerto alternativo)
    environment:
      - RABBITMQ_ERLANG_COOKIE=SWQOKODSQALRPCLNMEQG
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin123
      - RABBITMQ_NODE_NAME=rabbit@rabbitmq-node2
    volumes:
      - rabbitmq-node2-data:/var/lib/rabbitmq
      - ./rabbitmq/cluster-entrypoint.sh:/usr/local/bin/cluster-entrypoint.sh
    entrypoint: ["/usr/local/bin/cluster-entrypoint.sh"]
    command: ["rabbitmq-server"]
    networks:
      - app-network
    depends_on:
      rabbitmq-node1:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "check_port_connectivity"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Nodo 3 - RabbitMQ Follower
  rabbitmq-node3:
    image: rabbitmq:3.12-management
    container_name: carpeta-rabbitmq-node3
    hostname: rabbitmq-node3
    ports:
      - "5674:5672"   # AMQP (puerto alternativo para host)
      - "15674:15672" # Management UI (puerto alternativo)
    environment:
      - RABBITMQ_ERLANG_COOKIE=SWQOKODSQALRPCLNMEQG
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin123
      - RABBITMQ_NODE_NAME=rabbit@rabbitmq-node3
    volumes:
      - rabbitmq-node3-data:/var/lib/rabbitmq
      - ./rabbitmq/cluster-entrypoint.sh:/usr/local/bin/cluster-entrypoint.sh
    entrypoint: ["/usr/local/bin/cluster-entrypoint.sh"]
    command: ["rabbitmq-server"]
    networks:
      - app-network
    depends_on:
      rabbitmq-node1:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "check_port_connectivity"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

volumes:
  rabbitmq-node1-data:
  rabbitmq-node2-data:
  rabbitmq-node3-data:

networks:
  app-network:
    driver: bridge
```

#### Script de Clustering (`cluster-entrypoint.sh`)

```bash
#!/bin/bash
set -e

# Iniciar RabbitMQ en background
rabbitmq-server -detached

# Esperar a que RabbitMQ est√© listo
rabbitmq-diagnostics -q ping
rabbitmq-diagnostics -q check_port_connectivity

# Unirse al cluster del nodo 1
rabbitmqctl stop_app
rabbitmqctl reset
rabbitmqctl join_cluster rabbit@rabbitmq-node1
rabbitmqctl start_app

# Esperar indefinidamente (mantener contenedor vivo)
tail -f /dev/null
```

#### Configuraci√≥n de Quorum Queues en Spring Boot

```java
@Configuration
public class RabbitMQQuorumConfig {

    public static final String EXCHANGE_NAME = "documento.events";

    // Queues con Quorum habilitado
    public static final String DELETION_QUEUE = "documento.deletion.queue";
    public static final String MINIO_CLEANUP_QUEUE = "minio.cleanup.queue";
    public static final String METADATA_CLEANUP_QUEUE = "metadata.cleanup.queue";

    @Bean
    public TopicExchange documentoExchange() {
        return ExchangeBuilder
            .topicExchange(EXCHANGE_NAME)
            .durable(true)
            .build();
    }

    /**
     * Quorum Queue para solicitudes de eliminaci√≥n.
     * - x-queue-type=quorum: Activa Quorum Queue (Raft consensus)
     * - x-quorum-initial-group-size=3: Cluster de 3 nodos
     * - x-delivery-limit=3: Despu√©s de 3 reintentos, mover a DLQ
     */
    @Bean
    public Queue deletionQueue() {
        return QueueBuilder
            .durable(DELETION_QUEUE)
            .withArgument("x-queue-type", "quorum")  // üîë Quorum Queue
            .withArgument("x-quorum-initial-group-size", 3)  // üîë 3 nodos
            .withArgument("x-delivery-limit", 3)
            .withArgument("x-dead-letter-exchange", EXCHANGE_NAME)
            .withArgument("x-dead-letter-routing-key", "documento.deletion.dlq")
            .withArgument("x-message-ttl", 3600000) // 1 hora
            .build();
    }

    @Bean
    public Queue minioCleanupQueue() {
        return QueueBuilder
            .durable(MINIO_CLEANUP_QUEUE)
            .withArgument("x-queue-type", "quorum")
            .withArgument("x-quorum-initial-group-size", 3)
            .withArgument("x-delivery-limit", 3)
            .withArgument("x-dead-letter-exchange", EXCHANGE_NAME)
            .withArgument("x-dead-letter-routing-key", "minio.cleanup.dlq")
            .build();
    }

    @Bean
    public Queue metadataCleanupQueue() {
        return QueueBuilder
            .durable(METADATA_CLEANUP_QUEUE)
            .withArgument("x-queue-type", "quorum")
            .withArgument("x-quorum-initial-group-size", 3)
            .withArgument("x-delivery-limit", 3)
            .withArgument("x-dead-letter-exchange", EXCHANGE_NAME)
            .withArgument("x-dead-letter-routing-key", "metadata.cleanup.dlq")
            .build();
    }

    /**
     * Dead Letter Queue (NO necesita ser Quorum)
     * Los mensajes fallidos se almacenan aqu√≠ para revisi√≥n manual.
     */
    @Bean
    public Queue deletionDLQ() {
        return QueueBuilder.durable("documento.deletion.dlq").build();
    }

    // Bindings (id√©nticos a ADR-0003)
    @Bean
    public Binding deletionBinding() {
        return BindingBuilder
            .bind(deletionQueue())
            .to(documentoExchange())
            .with("documento.deletion.requested");
    }
}
```

#### Configuraci√≥n de Connection Factory con Load Balancing

```java
@Configuration
public class RabbitMQConnectionConfig {

    @Value("${spring.rabbitmq.addresses}")
    private String addresses;  // "rabbitmq-node1:5672,rabbitmq-node2:5672,rabbitmq-node3:5672"

    @Value("${spring.rabbitmq.username}")
    private String username;

    @Value("${spring.rabbitmq.password}")
    private String password;

    @Bean
    public ConnectionFactory connectionFactory() {
        CachingConnectionFactory factory = new CachingConnectionFactory();
        
        // Configurar m√∫ltiples addresses para failover autom√°tico
        factory.setAddresses(addresses);
        factory.setUsername(username);
        factory.setPassword(password);
        
        // Publisher Confirms para garantizar entrega
        factory.setPublisherConfirmType(ConfirmType.CORRELATED);
        factory.setPublisherReturns(true);
        
        // Connection recovery autom√°tico
        factory.setAutomaticRecoveryEnabled(true);
        factory.setNetworkRecoveryInterval(5000);  // 5 segundos
        
        return factory;
    }
}
```

#### application-docker.yml

```yaml
spring:
  rabbitmq:
    # Lista de nodos separados por coma (failover autom√°tico)
    addresses: rabbitmq-node1:5672,rabbitmq-node2:5672,rabbitmq-node3:5672
    username: admin
    password: admin123
    
    # Publisher Confirms (garantiza que RabbitMQ recibi√≥ el mensaje)
    publisher-confirm-type: correlated
    publisher-returns: true
    
    # Template configuration
    template:
      mandatory: true  # Lanza excepci√≥n si mensaje no puede enrutarse
      
    # Listener configuration
    listener:
      simple:
        acknowledge-mode: manual  # ACK manual para control fino
        prefetch: 10  # Procesar max 10 mensajes concurrentemente
        retry:
          enabled: true
          max-attempts: 3
          initial-interval: 1000ms
          multiplier: 2.0
          max-interval: 10000ms
```

## Consecuencias

### Positivas

- ‚úÖ **RNF-01 (Disponibilidad 99.99%)**: Cluster puede perder 1 nodo sin interrupci√≥n de servicio
- ‚úÖ **RNF-03 (RPO < 5 minutos)**: Mensajes replicados en 2 nodos; sin p√©rdida en fallo de 1 nodo
- ‚úÖ **RNF-04 (Tolerancia a Fallos)**: Sistema resiliente a fallo de un nodo completo del cluster
- ‚úÖ **RNF-09 (Escalado Horizontal)**: Puede agregar m√°s consumers para aumentar throughput
- ‚úÖ **Consistencia Fuerte**: Raft garantiza que todos los nodos tienen la misma vista de los mensajes
- ‚úÖ **Failover Autom√°tico**: Elecci√≥n de nuevo l√≠der en <5 segundos sin intervenci√≥n manual
- ‚úÖ **Poison Message Handling**: Quorum queues rastrean delivery counts a nivel de cluster (no se resetea en failover)
- ‚úÖ **Simplicidad Operacional**: No requiere configuraci√≥n de HA Mirroring (deprecado)

### Negativas

- ‚ö†Ô∏è **Overhead de Replicaci√≥n**: ~20-30% m√°s latencia vs classic queue (consenso Raft)
- ‚ö†Ô∏è **Consumo de Disco**: 2x almacenamiento (replication factor 2)
- ‚ö†Ô∏è **Consumo de Memoria**: M√°s RAM requerida para mantener Raft log en cada nodo
- ‚ö†Ô∏è **Complejidad de Cluster**: 3 nodos requieren m√°s recursos y coordinaci√≥n que 1 nodo
- ‚ö†Ô∏è **Incompatibilidad con Features Antiguas**: No soporta priority queues ni message TTL a nivel individual

### Riesgos

- üî¥ **Split Brain**: Si la red se particiona, podr√≠a haber 2 sub-clusters
    - **Mitigaci√≥n**: Raft previene split brain; solo el cluster con mayor√≠a (2+ nodos) puede aceptar escrituras

- üî¥ **P√©rdida de 2+ Nodos Simult√°neos**: Si fallan 2 o m√°s nodos, el cluster se vuelve read-only
    - **Mitigaci√≥n**: Monitoreo proactivo con alertas, backups autom√°ticos de queues

- üî¥ **Performance Degradation con Nodos Lentos**: Un nodo lento puede afectar consenso
    - **Mitigaci√≥n**: Monitorear latencia de replicaci√≥n, usar hardware homog√©neo

- üî¥ **Disco Lleno en un Nodo**: RabbitMQ bloquea publishers si un nodo no tiene espacio
    - **Mitigaci√≥n**: Alertas de uso de disco > 80%, configurar disk free limit

## Alternativas Consideradas

### Opci√≥n 1: Classic Queues con HA Mirroring (Deprecado)
- **Rechazo**: Deprecado en RabbitMQ 3.8+, menor consistencia, m√°s complejo de configurar

### Opci√≥n 2: Quorum Queues con 5 Nodos y RF=3
- **Rechazo**: Overhead excesivo de consenso Raft, mayor costo de infraestructura, latencia m√°s alta

### Opci√≥n 3: Streams de RabbitMQ
- **Rechazo**: Dise√±ados para persistencia a largo plazo (log-based), no para queues de trabajo con ACK

### Opci√≥n 4: Migrar a Apache Kafka
- **Rechazo**: Overkill para este caso de uso; RabbitMQ Quorum Queues son suficientes y m√°s simples

## M√©tricas y Monitoreo

### M√©tricas Clave de Cluster

```java
@Component
public class RabbitMQClusterMetrics {

    private final MeterRegistry meterRegistry;
    private final RestTemplate restTemplate;

    @Scheduled(fixedDelay = 60000)  // Cada 1 minuto
    public void captureClusterMetrics() {
        // Nodos activos en el cluster
        meterRegistry.gauge("rabbitmq.cluster.nodes.active", getActiveNodesCount());
        
        // Estado de quorum por queue
        meterRegistry.gauge("rabbitmq.quorum.members.online", 
            () -> getQuorumMembersOnline("documento.deletion.queue"));
        
        // Mensajes no confirmados (unacked)
        meterRegistry.gauge("rabbitmq.queue.messages.unacked",
            () -> getUnackedMessages("documento.deletion.queue"));
    }
}
```

### Alertas Cr√≠ticas

1. **Cluster con <3 nodos activos**: Indica p√©rdida de un nodo
2. **Quorum Queue con <2 members online**: Sin replicaci√≥n, riesgo de p√©rdida de datos
3. **Raft election timeout**: Indica problemas de red o latencia entre nodos
4. **Disk usage >85% en cualquier nodo**: Riesgo de bloqueo de publishers

## Validaci√≥n y Testing

### Test de Failover

```bash
# 1. Publicar 1000 mensajes
for i in {1..1000}; do
  curl -u admin:admin123 -X POST http://localhost:15672/api/exchanges/%2F/documento.events/publish \
    -H "Content-Type: application/json" \
    -d '{"routing_key":"documento.deletion.requested","payload":"test message '$i'"}'
done

# 2. Detener nodo l√≠der
docker stop carpeta-rabbitmq-node1

# 3. Verificar elecci√≥n de nuevo l√≠der (<5 segundos)
docker exec carpeta-rabbitmq-node2 rabbitmqctl cluster_status

# 4. Verificar que NO se perdieron mensajes
# (Los consumers deben poder seguir consumiendo)
```

### Test de Recuperaci√≥n

```bash
# 1. Detener nodo
docker stop carpeta-rabbitmq-node2

# 2. Esperar 30 segundos
sleep 30

# 3. Reiniciar nodo
docker start carpeta-rabbitmq-node2

# 4. Verificar re-sincronizaci√≥n autom√°tica
docker exec carpeta-rabbitmq-node2 rabbitmqctl list_queues name type state
```

## Implementaci√≥n Faseada

### Fase 1: Configuraci√≥n de Cluster (Semana 1)
- [x] Actualizar docker-compose.yml con 3 nodos
- [x] Crear script de clustering (`cluster-entrypoint.sh`)
- [x] Validar formaci√≥n de cluster con `rabbitmqctl cluster_status`

### Fase 2: Migraci√≥n a Quorum Queues (Semana 2)
- [ ] Actualizar RabbitMQConfig con `x-queue-type=quorum`
- [ ] Actualizar ConnectionFactory con m√∫ltiples addresses
- [ ] Testing de failover autom√°tico

### Fase 3: Monitoreo y Observabilidad (Semana 3)
- [ ] Implementar m√©tricas de cluster
- [ ] Configurar alertas para nodos down
- [ ] Dashboard Grafana con m√©tricas de Raft

### Fase 4: Producci√≥n (Semana 4)
- [ ] Load testing con 10K mensajes/segundo
- [ ] Chaos engineering (kill nodos aleatoriamente)
- [ ] Documentaci√≥n operacional completa

## Referencias

- [RabbitMQ Quorum Queues](https://www.rabbitmq.com/quorum-queues.html)
- [Raft Consensus Algorithm](https://raft.github.io/)
- [RabbitMQ Clustering Guide](https://www.rabbitmq.com/clustering.html)
- [Spring AMQP - Quorum Queues](https://docs.spring.io/spring-amqp/reference/amqp/resilience-recovering-from-errors-and-broker-failures.html)
- ADR-0003: Eliminaci√≥n de Documentos con Arquitectura Event-Driven usando RabbitMQ
- RNF-01, RNF-03, RNF-04, RNF-06, RNF-08, RNF-09: Requisitos no funcionales

## Notas Adicionales

1. **Erlang Cookie**: Usar el mismo cookie (`RABBITMQ_ERLANG_COOKIE`) en todos los nodos para permitir clustering
2. **Hostnames Estables**: Los nodos deben tener hostnames DNS resolvibles para clustering
3. **Network Latency**: Quorum queues son sensibles a latencia de red; mantener nodos en la misma regi√≥n
4. **Backup Strategy**: Considerar snapshots peri√≥dicos de `/var/lib/rabbitmq` para disaster recovery
5. **Upgrade Path**: Actualizar RabbitMQ nodo por nodo para mantener disponibilidad durante upgrades

---

**Fecha**: 2025-11-05  
**Autores**: Equipo Carpeta Ciudadana  
**Revisores**: Pendiente
